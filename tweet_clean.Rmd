---
title: "Creating Main Tweet (USA) dataframe"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggmap)
library(maps)
library(leaflet)
```

Making datasets less than 100MB just wit `drop_na`

```{r, eval = FALSE}
read_csv("./datasets/hashtag_donaldtrump.csv") %>% 
  drop_na() %>%
  slice(seq(0.5 * n())) %>% 
  write.csv("./datasets/trump1.csv")

read_csv("./datasets/hashtag_donaldtrump.csv") %>% 
  drop_na() %>%
  slice(-seq(0.5 * n())) %>% 
  write.csv("./datasets/trump2.csv")
  
read_csv("./datasets/hashtag_joebiden.csv") %>% 
  drop_na() %>% 
  slice(seq(0.5 * n())) %>% 
  write.csv("./datasets/biden1.csv")

read_csv("./datasets/hashtag_joebiden.csv") %>% 
  drop_na() %>% 
  slice(-seq(0.5 * n())) %>% 
  write.csv("./datasets/biden2.csv")
```

Importing datasets and making dates "tidy-er"

```{r, eval = TRUE}
trump_df = 
  merge(
    read_csv("./datasets/trump1.csv"),
    read_csv("./datasets/trump2.csv"),
    all = TRUE
  ) %>%
  select(!X1) %>% 
  separate(created_at, into = c("creation_date", "creation_time"), sep = " ") %>% 
  separate(creation_date, into = c("creation_year", "creation_month", "creation_day"), sep = "-") %>% 
  separate(user_join_date, into = c("join_date", "join_time"), sep = " ") %>% 
  separate(join_date, into = c("join_year", "join_month", "join_day"), sep = "-") %>% 
  mutate(hashtag = "Trump")

biden_df = 
  merge(
    read_csv("./datasets/biden1.csv"),
    read_csv("./datasets/biden2.csv"),
    all = TRUE
  ) %>%
  select(!X1) %>%  
  separate(created_at, into = c("creation_date", "creation_time"), sep = " ") %>% 
  separate(creation_date, into = c("creation_year", "creation_month", "creation_day"), sep = "-") %>% 
  separate(user_join_date, into = c("join_date", "join_time"), sep = " ") %>% 
  separate(join_date, into = c("join_year", "join_month", "join_day"), sep = "-") %>% 
  mutate(hashtag = "Biden")
```

Description of the dfs:
date of tweet (y,m,d), tweet_id (may be issues with column format), tweet, # of likes, reshares, user, followers, location (city, country, state, lat-long)

Joining dfs and subsetting for US

```{r}
tweets = 
  merge(biden_df, trump_df, all = TRUE)

# Subset with USA tweets only
tweets_usa =
  merge(biden_df, trump_df, all = TRUE) %>% 
  filter(country == "United States of America")
```

Election stuff

```{r}
election_df =
  read_csv("./datasets/president_county_candidate.csv") %>% 
  group_by(state, party) %>% 
  mutate(party_total = sum(total_votes)) %>% 
  ungroup() %>% 
  group_by(state) %>%
  mutate(state_winner = case_when(
                      party_total == max(party_total) ~ TRUE,
                      party_total != max(party_total) ~ FALSE),
         state_total = sum(total_votes)
  )

state_election_df = 
  election_df %>% 
  filter(state_winner == TRUE) %>% 
  select(state, candidate, party, party_total, state_total) %>% 
  distinct()
```

joining df

```{r}
main_tweets_usa = 
  left_join(tweets_usa, state_election_df, by = "state") %>% 
  rename(
    winner_candidate = candidate,
    winner_party = party
  )
```

[1] 16628 tweets contain #trump AND #biden:

```{r}
nrow(main_tweets_usa) - main_tweets_usa %>% 
  distinct(tweet) %>% 
  nrow()
```

## Plots
Characterizing tweets
TODO : use the same colors

```{r}
tweets_usa %>%
group_by(state, hashtag)%>%
summarise(count = n()) %>%  
ggplot() +
geom_col(aes(x=state, y=count, color = hashtag ), position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs (title = "Distribution of tweets across US states")
```

for NYC map over boroughs + plot them over map (map is not showing up)
```{r}

tweets_usa %>% 
 filter (state_code == 'NY' & city == 'New York') %>%
  leaflet() %>% 
  addTiles() %>%
  addProviderTiles(providers$CartoDB.Positron) %>% 
  addCircleMarkers(~lat, ~long, radius = 2, color = ~hashtag)

```


```{r}
tweets_usa %>% 
mutate(date = as.Date(paste(creation_month, creation_day, '2020', sep = "/"), format  = "%m/%d/%y")) %>%  
group_by (date, hashtag) %>%
summarise(count = n())  %>%
ggplot(aes(x=date,y=count,color = hashtag)) +
geom_smooth(method = "lm") +
geom_point(aes(x=date, y=count, color = hashtag)) +
geom_vline(xintercept = as.Date("2020-11-02")) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
scale_x_date(date_breaks = '1 day') +
labs (title = "Distribution of tweets over time", 
        x = "Day",
        y = "Number of tweets")
```

We see that there is an increase in tweets over time with Biden having less tweets overall (albeit not statistically significant)


```{r}

#tweets_usa %>%
#select (user_followers_count) %>%
#  arrange(desc(user_followers_count))

tweets_usa %>%
group_by(hashtag)%>%
filter (retweet_count>0 & likes>0) %>%
filter (user_followers_count<=3750110) %>%# filter outliers  
mutate(user_followers_count = user_followers_count/1000) %>%
ggplot() +
geom_point(aes(x= user_followers_count , y= retweet_count, color = hashtag, size = likes  )) +
  labs (title = "How tweets are shared and liked depending on the number of followers", 
        x = "Numer of users, thousands",
        y = "Number of retweets")
```
 



```{r}

usa_map <- map_data("state")

tweet_map <- tweets_usa %>%
group_by(state, hashtag) %>%
summarise(count = n(),
          likes = sum(likes)) %>%  
mutate (likes_tweets = likes*count,
        region = tolower(state)) %>%
select (region, hashtag, likes_tweets)  %>%
pivot_wider(names_from = "hashtag",
            values_from = "likes_tweets")  %>%
mutate(top = case_when(Biden>coalesce(Trump,0) ~ "Biden",
                       Trump>Biden ~ "Trump"))


states_tweet_map <- left_join(usa_map, tweet_map)


p <- ggplot(data = states_tweet_map,
            aes(x = long, y = lat,
                group = group, fill = top))

p + geom_polygon(color = "gray90", size = 0.1) +
    labs(title ="Most popular tweets across states")
  
```




