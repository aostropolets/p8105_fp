---
title: "P8105 - Tweet cleaning"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
```

Making datasets less than 100MB just wit `drop_na`

```{r, eval = FALSE}
read_csv("./datasets/hashtag_donaldtrump.csv") %>% 
  drop_na() %>%
  slice(seq(0.5 * n())) %>% 
  write.csv("./datasets/trump1.csv")

read_csv("./datasets/hashtag_donaldtrump.csv") %>% 
  drop_na() %>%
  slice(-seq(0.5 * n())) %>% 
  write.csv("./datasets/trump2.csv")
  
read_csv("./datasets/hashtag_joebiden.csv") %>% 
  drop_na() %>% 
  slice(seq(0.5 * n())) %>% 
  write.csv("./datasets/biden1.csv")

read_csv("./datasets/hashtag_joebiden.csv") %>% 
  drop_na() %>% 
  slice(-seq(0.5 * n())) %>% 
  write.csv("./datasets/biden2.csv")
```

Importing datasets and making dates "tidy-er"

```{r}
trump_df = 
  merge(
    read_csv("./datasets/trump1.csv"),
    read_csv("./datasets/trump2.csv"),
    all = TRUE
  ) %>%
  select(!X1) %>% 
  separate(created_at, into = c("creation_date", "creation_time"), sep = " ") %>% 
  separate(creation_date, into = c("creation_year", "creation_month", "creation_day"), sep = "-") %>% 
  separate(user_join_date, into = c("join_date", "join_time"), sep = " ") %>% 
  separate(join_date, into = c("join_year", "join_month", "join_day"), sep = "-") %>% 
  mutate(hashtag = "Trump")

biden_df = 
  merge(
    read_csv("./datasets/biden1.csv"),
    read_csv("./datasets/biden2.csv"),
    all = TRUE
  ) %>%
  select(!X1) %>%  
  separate(created_at, into = c("creation_date", "creation_time"), sep = " ") %>% 
  separate(creation_date, into = c("creation_year", "creation_month", "creation_day"), sep = "-") %>% 
  separate(user_join_date, into = c("join_date", "join_time"), sep = " ") %>% 
  separate(join_date, into = c("join_year", "join_month", "join_day"), sep = "-") %>% 
  mutate(hashtag = "Biden")
```

Joining dfs

```{r}
tweets = 
  merge(biden_df, trump_df, all = TRUE)
```

Creating cleaned text version of tweets

```{r}
biden_text = 
  biden_df %>% 
  select(tweet) %>%
  mutate(
    tweet = gsub("#[[:alpha:]]*", "", tweet),
    tweet = gsub("@[[:alpha:]]*", "", tweet),
    tweet = gsub("https\\S*", "", tweet),
    tweet = gsub("http\\S*", "", tweet),
    tweet = gsub("@\\S*", "", tweet),
    tweet = gsub("amp", "", tweet),
    tweet = gsub("[\r\n]", "", tweet),
    tweet = gsub("[0-9]", "", tweet),
    tweet = gsub("[[:punct:]]", "", tweet)
  ) %>% 
  slice_head(n = 10000)
```

```{r}
trump_text = 
  trump_df %>% 
  select(tweet) %>% 
  mutate(
    tweet = gsub("#[[:alpha:]]*", "", tweet),
    tweet = gsub("@[[:alpha:]]*", "", tweet),
    tweet = gsub("https\\S*", "", tweet),
    tweet = gsub("http\\S*", "", tweet),
    tweet = gsub("@\\S*", "", tweet),
    tweet = gsub("amp", "", tweet),
    tweet = gsub("[\r\n]", "", tweet),
    tweet = gsub("[0-9]", "", tweet),
    tweet = gsub("[[:punct:]]", "", tweet)
  ) %>% 
  slice_sample(n = 10000)
```

Useful stuff:

tidytext::unnest_tokens() -> unique words

https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/text-mining-twitter-data-intro-r/

## WordClouds

Also generating a _unique_word_count_ dataframe

```{r}
library(tm)
library(wordcloud) #option1
library(wordcloud2) #option2

# Create a corpus  
docs1 =
  Corpus(VectorSource(biden_text)) %>% 
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace) %>% 
  tm_map(content_transformer(tolower)) %>% 
  tm_map(removeWords, stopwords("english"))

docs2 =
  Corpus(VectorSource(trump_text)) %>% 
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace) %>% 
  tm_map(content_transformer(tolower)) %>% 
  tm_map(removeWords, stopwords("english"))

words1 = 
  TermDocumentMatrix(docs1) %>% 
  as.matrix() %>% 
  rowSums() %>% 
  sort(decreasing = TRUE)

words2 = 
  TermDocumentMatrix(docs2) %>% 
  as.matrix() %>% 
  rowSums() %>% 
  sort(decreasing = TRUE)

word_count_biden = data.frame(word = names(words1),freq = words1)
word_count_trump = data.frame(word = names(words2),freq = words2)

#Option1
wordcloud(words = word_count_biden$word, freq = word_count_biden$freq, min.freq = 2,           
          max.words = 200, random.order = FALSE, rot.per = 0.45,            
          colors = brewer.pal(8, "Dark2"))

wordcloud(words = word_count_trump$word, freq = word_count_trump$freq, min.freq = 2,           
          max.words = 200, random.order = FALSE, rot.per = 0.45,            
          colors = brewer.pal(8, "Dark2"))

#Option2
wordcloud2(data = word_count_biden, size = 1.5, color = 'random-dark', ellipticity = 1)

wordcloud2(data = word_count_trump, size = 1.5, color = 'random-dark', ellipticity = 1)
```

